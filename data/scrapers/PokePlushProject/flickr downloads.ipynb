{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOx8SLgWDY30kwQ8baXkBvR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# find flickr user's id\n","import flickrapi\n","\n","API_KEY = \"90955b22fa72536d02e0da876d635760\"\n","API_SECRET = \"6eefed0dc7a85c9d\"\n","USERNAME = \"pokeplushproject\"\n","\n","flickr = flickrapi.FlickrAPI(API_KEY, API_SECRET, format='parsed-json')\n","response = flickr.people.findByUsername(username=USERNAME)\n","USER_ID = response['user']['nsid']\n","\n","print(\"User NSID:\", USER_ID)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DQtmFgg0skD4","executionInfo":{"status":"ok","timestamp":1756066197887,"user_tz":240,"elapsed":159,"user":{"displayName":"Varen Maniktala","userId":"06075269516028890002"}},"outputId":"22bf6d80-0e76-4aa5-defe-ddb61b6d42f7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["User NSID: 23662195@N06\n"]}]},{"cell_type":"code","source":["# --- Step 0: User chooses mode ---\n","print(\"Choose mode:\")\n","print(\"1: Download images from URL file only\")\n","print(\"2: Check API for missing image URLs and download all\")\n","mode = input(\"Enter 1 or 2: \").strip()\n","if mode not in (\"1\", \"2\"):\n","    raise ValueError(\"Invalid mode! Enter 1 or 2.\")\n","\n","# --- Step 1: Mount Google Drive ---\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import requests\n","import flickrapi\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import shutil\n","from datetime import datetime\n","import time\n","\n","# --- Step 2: Flickr API setup ---\n","API_KEY = \"90955b22fa72536d02e0da876d635760\"\n","API_SECRET = \"6eefed0dc7a85c9d\"\n","USER_ID = \"23662195@N06\"\n","\n","flickr = flickrapi.FlickrAPI(API_KEY, API_SECRET, format='parsed-json')\n","\n","# --- Step 3: Paths and logging ---\n","BASE_DIR = \"/content/drive/MyDrive/IRL Pokedex/training images (plushies)\" # this is the destination folder for all downlaods\n","os.makedirs(BASE_DIR, exist_ok=True)\n","\n","LOG_FILE = os.path.join(BASE_DIR, \"flickr_downloads_log.txt\")\n","URL_FILE = os.path.join(BASE_DIR, \"flickr_downloads_urls.txt\")\n","\n","api_call_count = 0  # global counter\n","\n","def log(message):\n","    line = f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {message}\"\n","    print(line)\n","    with open(LOG_FILE, \"a\") as f:\n","        f.write(line + \"\\n\")\n","\n","def save_url(photo_id, url, filepath):\n","    with open(URL_FILE, \"a\") as f:\n","        f.write(f\"{photo_id}\\t{url}\\t{filepath}\\n\")\n","\n","def safe_filename(name):\n","    return \"\".join(c if c.isalnum() or c in \" ._-\" else \"_\" for c in name)\n","\n","def api_call(func, *args, **kwargs):\n","    global api_call_count\n","    result = func(*args, **kwargs)\n","    api_call_count += 1\n","    log(f\"API call #{api_call_count}: {func.__name__}\")\n","    time.sleep(1)\n","    return result\n","\n","def get_original_url(photo_id):\n","    sizes = api_call(flickr.photos.getSizes, photo_id=photo_id)\n","    for size in sizes['sizes']['size']:\n","        if size['label'] == \"Original\":\n","            return size['source']\n","    return sizes['sizes']['size'][-1]['source']\n","\n","# --- Step 4: Load existing URL file ---\n","existing_urls = {}\n","if os.path.exists(URL_FILE):\n","    with open(URL_FILE, \"r\") as f:\n","        for line in f:\n","            parts = line.strip().split(\"\\t\")\n","            if len(parts) == 3:\n","                existing_urls[parts[0]] = (parts[1], parts[2])\n","\n","download_jobs = []\n","\n","# --- Step 5: Mode 2: Check API for missing URLs (both album/photostream) ---\n","if mode == \"2\":\n","    album_photo_ids = set(existing_urls.keys())\n","    album_photo_count = 0\n","    photostream_photo_count = 0\n","\n","    photosets = api_call(flickr.photosets.getList, user_id=USER_ID)\n","    album_count = len(photosets['photosets']['photoset'])\n","    log(f\"Found {album_count} albums.\")\n","\n","    for album in photosets['photosets']['photoset']:\n","        album_title = safe_filename(album['title']['_content']) or f\"album_{album['id']}\"\n","        album_dir = os.path.join(BASE_DIR, album_title)\n","        os.makedirs(album_dir, exist_ok=True)\n","\n","        log(f\"Scanning album: {album_title}\")\n","        photos = api_call(flickr.photosets.getPhotos, photoset_id=album['id'], user_id=USER_ID)\n","\n","        for photo in photos['photoset']['photo']:\n","            photo_id = photo['id']\n","            album_photo_ids.add(photo_id)\n","            if photo_id not in existing_urls:\n","                album_photo_count += 1\n","                url = get_original_url(photo_id)\n","                filename = f\"{safe_filename(photo['title'])}_{photo_id}.jpg\"\n","                filepath = os.path.join(album_dir, filename)\n","                if not os.path.exists(filepath):\n","                  download_jobs.append((url, filepath))\n","                save_url(photo_id, url, filepath)\n","\n","    log(f\"Collected {album_photo_count} new photos from albums.\")\n","\n","    # Photostream\n","    log(\"Scanning photostream...\")\n","    page = 1\n","    while True:\n","        photostream = api_call(flickr.people.getPhotos, user_id=USER_ID, per_page=500, page=page)\n","        photos = photostream['photos']['photo']\n","\n","        if not photos:\n","            log(f\"No photos returned on page {page}, stopping.\")\n","            break\n","\n","        # Process photos\n","        for photo in photos:\n","            photo_id = photo['id']\n","            if photo_id not in album_photo_ids and photo_id not in existing_urls:\n","                photostream_photo_count += 1\n","                url = get_original_url(photo_id)\n","                folder = os.path.join(BASE_DIR, f\"photo_{photo_id}\")\n","                os.makedirs(folder, exist_ok=True)\n","                filename = f\"{safe_filename(photo['title'])}_{photo_id}.jpg\"\n","                filepath = os.path.join(folder, filename)\n","                if not os.path.exists(filepath):\n","                    download_jobs.append((url, filepath))\n","                save_url(photo_id, url, filepath)\n","\n","        if page >= photostream['photos']['pages']:\n","            log(f\"Reached last page ({page}), stopping photostream scan.\")\n","            break\n","\n","        page += 1\n","\n","    log(f\"Collected {photostream_photo_count} new photos from photostream.\")\n","    log(f\"Total new photos to download: {len(download_jobs)}\")\n","\n","# --- Step 6: Mode 1 or resume: Download from URL file ---\n","if mode == \"1\" or mode == \"2\":\n","    if not download_jobs:\n","        log(\"No new downloads found. Checking URL file for existing downloads...\")\n","        for photo_id, (url, filepath) in existing_urls.items():\n","            if not os.path.exists(filepath):\n","                download_jobs.append((url, filepath))\n","        log(f\"{len(download_jobs)} photos need downloading from URL file.\")\n","\n","# --- Step 7: Parallel Download ---\n","def download_file(job):\n","    url, filepath = job\n","    try:\n","        resp = requests.get(url, stream=True, timeout=30)\n","        resp.raise_for_status()\n","        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n","        with open(filepath, \"wb\") as f:\n","            for chunk in resp.iter_content(1024):\n","                f.write(chunk)\n","        time.sleep(0.3)  # short pause to reduce 429 risk\n","        return f\"Downloaded: {filepath}\"\n","    except Exception as e:\n","        return f\"Failed {filepath}: {e}\"\n","\n","if download_jobs:\n","    log(\"Starting downloads...\")\n","    with ThreadPoolExecutor(max_workers=2) as executor:\n","        futures = [executor.submit(download_file, job) for job in download_jobs]\n","        for future in as_completed(futures):\n","            log(future.result())\n","else:\n","    log(\"✅ No photos to download.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBmi4SZkCVpF","executionInfo":{"status":"ok","timestamp":1756075842544,"user_tz":240,"elapsed":14895,"user":{"displayName":"Varen Maniktala","userId":"06075269516028890002"}},"outputId":"d31d9828-e809-4cd9-9f5a-7a12ec9c201a"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Choose mode:\n","1: Download images from URL file only\n","2: Check API for missing image URLs and download all\n","Enter 1 or 2: 2\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[2025-08-24 22:50:29] Scanning photostream...\n","[2025-08-24 22:50:29] API call #1: getPhotos\n","[2025-08-24 22:50:31] API call #2: getPhotos\n","[2025-08-24 22:50:33] API call #3: getPhotos\n","[2025-08-24 22:50:34] API call #4: getPhotos\n","[2025-08-24 22:50:36] API call #5: getPhotos\n","[2025-08-24 22:50:37] API call #6: getPhotos\n","[2025-08-24 22:50:39] API call #7: getPhotos\n","[2025-08-24 22:50:40] API call #8: getPhotos\n","[2025-08-24 22:50:41] Reached last page (8), stopping photostream scan.\n","[2025-08-24 22:50:41] Collected 0 new photos from photostream.\n","[2025-08-24 22:50:41] Total new photos to download: 0\n","[2025-08-24 22:50:41] No new downloads found. Checking URL file for existing downloads...\n","[2025-08-24 22:50:42] 0 photos need downloading from URL file.\n","[2025-08-24 22:50:42] ✅ No photos to download.\n"]}]}]}